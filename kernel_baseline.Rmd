---
title: "Tweet Sentiment Extraction"
subtitle: "Baseline model in R"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, result=FALSE, cache=FALSE)
library(icon)
knitr::knit_hooks$set(
  source = function(x, options) {
    hook.r = function(x, options) {
      fence <- "```"
      language = tolower(options$engine)
      if (language == 'node') language = 'javascript'
      if (!options$highlight) language = 'text'
      if(!is.null(options$foldcode)) {
      paste0('\n\n', "<details><summary>Source</summary>\n", fence, language, '\n', x, fence,  '\n\n', "</details>\n")
      } else {
              paste0('\n\n', fence, language, '\n', x, fence,  '\n\n')
      }
    }
    x = knitr:::hilight_source(x, 'markdown', options)
    hook.r(
      paste(c(
        x, 
        ''
      ), collapse = '\n'), 
      options
    )
  }
)
```

Author: [Fellipe Gomes](https://github.com/gomesfellipe) (Statistics - UFF, Data Scientist - FGV IBRE / BRAZIL) 

<p align="right"><span style="color:firebrick">If you liked the kernel don't forget the upvote! `r icon::fa("hand-peace")`</span> </p>


# Problem Definition

This competition presents a slightly different challenge when we talking about sentiment analysis. Usually we have a dataset with some texts labeled as sentiments (negative, neutral and positive) and we develop a model that is able to estimate the sentiment of a text that is not labeled.

Here we are interested in predicting the sequence of words that best supports the feeling of the tweet (including punctuation, etc). In addition, the text can only be divided by white space. This brings up the reflection on whether it is a big deal to apply the traditional methods of cleaning and analyzing data for the application of machine learning models or whether we will have even more interesting results using alternative approaches like [NER](https://en.wikipedia.org/wiki/Named-entity_recognition) or slightly more complex approaches.

I never carried out an analysis of this in my job so before I started developing the baseline I did a quick "literature review" to find different approaches to solve this type of task and found some options like [Question Answering](https://web.stanford.edu/class/cs124/lec/watsonqa.pdf) (with [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), [XLNET](https://www.google.com/search?q=XLNET&oq=XLNET&aqs=chrome..69i57j0l6j69i60.499j0j4&sourceid=chrome&ie=UTF-8), [XLM](https://github.com/facebookresearch/XLM), or [DistilBERT](https://arxiv.org/abs/1910.01108)) or [Text summarization](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/lecture-notes/lec25.pdf) approache.

After submitting the baseline model, I will develop new kernels with some friends to evaluate new models using R and Python!

I hope you enjoy it! `r icon::fa("grin-beam")`

# Available data

Loading train dataset and packages used in the kernel:

```{r}
# packages
library(readr)      # read/write
library(dplyr)      # manipulate data
library(tidyr)      # tidy data
library(purrr)      # functional programming
library(stringr)    # text manipulation
library(quanteda)   # textmining
library(knitr)      # print table
library(kableExtra) # custom table
library(ggplot2)    # elegant plot
library(gghalves)   # half plot
theme_set(theme_bw()) # set theme

# getting data:
train_data <- read_csv("data/train.csv")
source("functions.R")
```

Load functions developed in R to obtain some text metadata:

(Click " `r icon::fa("play")`Source" to show the code)

```{r, foldcode=TRUE}
# Version 1 Functions to competition: 
# kaggle.com/c/tweet-sentiment-extraction/

#' Custom Kabble plot
#' 
#' Custom kable to kaggle kernels
#' 
#' @param x dataframe
#' @return print a custom html table
#' @example 
#' kable2(mtcars)
kable2 <- function(x, height = "300px"){
  require(knitr)
  require(kableExtra)
  
  kable(x) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"),
                  full_width = F, position = "center") %>%
    scroll_box(width = "100%", height = height)
}

#' Jaccard
#'
#' function developed to solve the problem in kaggle
#' https://www.kaggle.com/c/tweet-sentiment-extraction/
#'
#' @param str1 string
#' @param str2 string
#' @return Jaccard metric
#' @examples
#' str1 <- "Ola cOmo Vai voce"
#' str2 <- "Ola cOmo vai "
#' jaccard(str1, str2)
#'
#' str1 <- c("a a b", "a b c", "a c d")
#' str2 <- c("a d b", "a e c", "a a e")
#' purrr::map2_dbl(str1, str2, ~ jaccard(.x, .y))
jaccard <- function(str1, str2) {
  # r version for: https://www.kaggle.com/c/tweet-sentiment-extraction/overview/evaluation
  a <- unlist(strsplit(tolower(str1), split = " "))
  b <- unlist(strsplit(tolower(str2), split = " "))
  c <- intersect(a, b)
  length(c) / (length(a) + length(b) - length(c))
}

#' Clean Tweets
#'
#' fucntion to clean general text from twitter
#'
#' @param x string
#' @return cleaned string
#' @example
#' clean_text("HavE a niCe DAy!!! :)$$$")
clean_text <- function(x) {
  require(dplyr)
  require(stringr)
  
  x %>%
    str_remove_all("(RT|via)((?:\\b\\W*@\\w+)+)") %>%
    str_to_lower() %>%
    str_remove_all("@\\w+") %>%
    str_remove_all("[[:digit:]]") %>%
    str_remove_all("http(s|).*") %>%
    str_remove_all("[ |\t]{2,}") %>%
    str_remove_all("w( |)/") %>%
    str_remove_all("(?! )[^[:alnum:]]") %>%
    str_remove_all("'\\b\\w{1,2}\\b'") %>%
    str_trim() %>%
    str_squish()
}

#' Get Metadata
#'
#' function developed to get metadata of twitter dataset
#' https://www.kaggle.com/c/tweet-sentiment-extraction/data
#'
#' @param x = load dataset
#' @return dataset dataset combined with metadata
#' @example
#' metadata <- get_metadata(train_data)
#'
# metadata
get_metadata <- function(x) {
  require(dplyr)
  require(stringr)
  require(quanteda)
  require(purrr)
  t0 <- Sys.time() # to print time
  cat("Getting metadata, please wait ..\n")
  
  x <-
    x %>%
    # rename selected_text to sel_text
    rename(sel_text = selected_text) %>%
    # features engineering
    mutate(
      # on text
      text_na = is.na(text),
      text_len = str_length(text),
      text_n_words = ntoken(text),
      text_npunc = str_count(text, "[[:punct:]]"),
      text_numbers = str_count(text, "[[:digit:]]"),
      text_links = str_count(text, "http(s|).*"),
      text_hashtags = str_count(text, "#\\w+"),
      text_retweet = str_count(text, "(RT|via)((?:\\b\\W*@\\w+)+)"),
      text_atpeople = str_count(text, "@\\w+"),
      text_clean = clean_text(text),
      # on sel_text
      sel_text_na = is.na(sel_text),
      sel_text_len = str_length(sel_text),
      sel_text_n_words = ntoken(sel_text),
      sel_text_npunc = str_count(sel_text, "[[:punct:]]"),
      sel_text_numbers = str_count(sel_text, "[[:digit:]]"),
      sel_text_links = str_count(sel_text, "http(s|).*"),
      sel_text_hashtags = str_count(sel_text, "#\\w+"),
      sel_text_retweet = str_count(sel_text, "(RT|via)((?:\\b\\W*@\\w+)+)"),
      sel_text_atpeople = str_count(sel_text, "@\\w+"),
      sel_text_clean = clean_text(sel_text),
      # mutual
      equal_texts = text == sel_text,
      jaccard = purrr::map2_dbl(text, sel_text, ~ jaccard(.x, .y)),
    ) %>%
    {
      # add columns start and end sel_text
      bind_cols(.,
                map2_dfr(
                  .$text, .$sel_text,
                  ~ { # include \\ before special characters before the search
                    .y <- str_replace_all(.y, "([[:punct:]]|\\*)", "\\\\\\1")
                    str_locate(.x, .y) %>%
                      as_tibble() } ) ) } %>%
    select(textID, text, sel_text, sentiment, start, end, jaccard, 
           text_clean, sel_text_clean, everything())
  
  cat(paste0("Metadata successfully obtained!\nThe process took: ",
             round(Sys.time()-t0) ," seconds")) # Yeah!
  
  return(x)
}
```

</br>

See the first 100 lines of the imported dataset:

```{r}
kable2(train_data[1:100,])
```

The training dataset provided has `r dim(train_data)[1]` rows and `r dim(train_data)[2]` columns.

# Getting Metadata

Use the function `get_metadata()` developed for feature engineering of this dataset (twitter texts):

```{r}
train_metadata <- get_metadata(train_data)
kable2(train_metadata[1:100,])
```

</br>

Check if there is an empty line:

```{r}
train_metadata %>% 
  filter(text_na | sel_text_na) %>% 
  kable2(height = NULL)
```

</br>

`r icon::fa("exclamation")` There are `r nrow(filter(train_metadata, text_na | sel_text_na))` empty lines, let's remove them:

```{r}
train_metadata <- train_metadata %>% filter(!text_na)
```

# Exploratory data analysis

Another kernel will be developed to make an appropriate exploratory analysis of texts and metadata. For now we are only looking for an overview of the data.

Let's see:

```{r}
train_metadata %>% 
  select(sentiment, jaccard, text_len:text_atpeople) %>% 
  gather(key, value, -sentiment) %>% 
  ggplot(aes(x = key, y = value, fill = sentiment))+
  geom_half_boxplot(side = "l", center = TRUE, errorbar.draw = FALSE)+
  geom_half_violin(side = "r")+
  facet_wrap(~key, scales = "free")+
  theme(legend.position = "botton")
```

Some insights after analyzing the metadata:

* Approximately 75% of texts with neutral sentiment show very high Jaccard similarity, which may indicate that the full text for submission can guarantee a metric around `r round(mean(filter(train_metadata, sentiment == "neutral")$jaccard), 4)` (mean of the jaccard similarity when` sentiment == neutral` in train data);
* The `text_atpeople`,` text_hashtags` `text_links` attributes are not present in most texts;
* The text size (both number of words and characters) is very similar in all cases;
* None of the texts have RT.

# Baseline model

Estimated score of jaccard similarity in training data using the full text:

```{r}
print(glue::glue("Jaccard similarity in training data: {round(mean(train_metadata$jaccard), 4)}"))
```

Save the submission file using full `text`column to obtain a reference jaccard metric as starting point for the development of more complex models:

```{r}
read_csv("data/test.csv") %>% 
  select(textID, selected_text = text) %>% 
  write_csv("data/baseline.csv")
```

`r icon::fa("check-circle")` Yeahh!! Lets go submmit!


